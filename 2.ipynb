{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [

      ],
      "metadata": {
        "id": "00B-D1PhDPiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT_xSHCE9hJT",
        "outputId": "71675bb3-3061-40fc-8c17-b891dba9426e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo9bTf0S8_uK",
        "outputId": "99664514-cf4d-4765-f94e-a3d6e413e2b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn gensim nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample data\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "# Create the CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_bow = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the result to a dense array\n",
        "dense_array_bow = X_bow.toarray()\n",
        "\n",
        "# Display the BoW matrix\n",
        "print(\"Bag-of-Words Matrix:\")\n",
        "print(dense_array_bow)\n",
        "\n",
        "# Display feature names\n",
        "print(\"\\nFeature Names:\")\n",
        "print(feature_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNrf31xK9LUx",
        "outputId": "2877201b-3240-4c66-b0ae-6cc6ff3a5d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-Words Matrix:\n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n",
            "\n",
            "Feature Names:\n",
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Term Frequency (TF)\n",
        "TF = dense_array_bow / dense_array_bow.sum(axis=1, keepdims=True)\n",
        "\n",
        "# Display normalized count occurrence (TF)\n",
        "print(\"\\nNormalized Count Occurrence (TF):\")\n",
        "print(TF)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETZIc_Am9PFB",
        "outputId": "c19c0b4f-3bf5-4f42-9a1f-dfc0c74af922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Normalized Count Occurrence (TF):\n",
            "[[0.         0.2        0.2        0.2        0.         0.\n",
            "  0.2        0.         0.2       ]\n",
            " [0.         0.33333333 0.         0.16666667 0.         0.16666667\n",
            "  0.16666667 0.         0.16666667]\n",
            " [0.16666667 0.         0.         0.16666667 0.16666667 0.\n",
            "  0.16666667 0.16666667 0.16666667]\n",
            " [0.         0.2        0.2        0.2        0.         0.\n",
            "  0.2        0.         0.2       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Create the TF-IDF transformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "\n",
        "# Fit and transform the TF matrix\n",
        "X_tfidf = tfidf_transformer.fit_transform(X_bow)\n",
        "\n",
        "# Convert the result to a dense array\n",
        "dense_array_tfidf = X_tfidf.toarray()\n",
        "\n",
        "# Display the TF-IDF matrix\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(dense_array_tfidf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L64aZVPb9Rvk",
        "outputId": "3ad6ce04-c824-4851-b6bc-0d2666bd29fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# Tokenize the documents\n",
        "tokenized_documents = [word_tokenize(doc.lower()) for doc in documents]\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=tokenized_documents, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Example: Get the embedding for the word 'document'\n",
        "embedding_document = word2vec_model.wv['document']\n",
        "print(\"\\nWord2Vec Embedding for 'document':\")\n",
        "print(embedding_document)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj8ZS7BK9WEW",
        "outputId": "85f5d0a9-6381-4735-f7b7-cae415be8d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word2Vec Embedding for 'document':\n",
            "[-5.3761393e-04  2.3459077e-04  5.1012170e-03  9.0115219e-03\n",
            " -9.3035055e-03 -7.1186870e-03  6.4577162e-03  8.9744031e-03\n",
            " -5.0161965e-03 -3.7644049e-03  7.3809391e-03 -1.5342169e-03\n",
            " -4.5370674e-03  6.5543531e-03 -4.8609949e-03 -1.8136933e-03\n",
            "  2.8776617e-03  9.8915887e-04 -8.2834894e-03 -9.4506554e-03\n",
            "  7.3119737e-03  5.0714435e-03  6.7562792e-03  7.6230383e-04\n",
            "  6.3530928e-03 -3.4065295e-03 -9.4848091e-04  5.7711215e-03\n",
            " -7.5222286e-03 -3.9373739e-03 -7.5092558e-03 -9.2885981e-04\n",
            "  9.5392875e-03 -7.3166536e-03 -2.3360765e-03 -1.9363161e-03\n",
            "  8.0779977e-03 -5.9297686e-03  4.5617318e-05 -4.7524953e-03\n",
            " -9.6023204e-03  5.0089518e-03 -8.7604597e-03 -4.3930719e-03\n",
            " -3.5214103e-05 -2.9548592e-04 -7.6621324e-03  9.6163880e-03\n",
            "  4.9832016e-03  9.2352722e-03 -8.1572160e-03  4.4980138e-03\n",
            " -4.1374546e-03  8.2675234e-04  8.4996261e-03 -4.4643688e-03\n",
            "  4.5164214e-03 -6.7876368e-03 -3.5471660e-03  9.3982853e-03\n",
            " -1.5784105e-03  3.2352045e-04 -4.1381051e-03 -7.6831253e-03\n",
            " -1.5093960e-03  2.4685122e-03 -8.8934030e-04  5.5310265e-03\n",
            " -2.7425813e-03  2.2589052e-03  5.4565049e-03  8.3474834e-03\n",
            " -1.4548642e-03 -9.2107793e-03  4.3709916e-03  5.6996895e-04\n",
            "  7.4426048e-03 -8.1429747e-04 -2.6364601e-03 -8.7528294e-03\n",
            " -8.5558271e-04  2.8258313e-03  5.4009468e-03  7.0547434e-03\n",
            " -5.7027498e-03  1.8572190e-03  6.0867225e-03 -4.7976980e-03\n",
            " -3.1054933e-03  6.7991368e-03  1.6290357e-03  1.9226066e-04\n",
            "  3.4747359e-03  2.1982045e-04  9.6214656e-03  5.0585950e-03\n",
            " -8.9198640e-03 -7.0399828e-03  9.0400706e-04  6.3935039e-03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}
