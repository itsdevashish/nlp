{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, TreebankWordTokenizer, TweetTokenizer, RegexpTokenizer, MWETokenizer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4q39MAo48Da",
        "outputId": "102ec2f2-2694-403a-d77b-04a750af9431"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PIqLV2G49CC",
        "outputId": "15a0d865-4840-4cb5-c7a3-3f5b19d8d0b1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"NLTK is a powerful library for natural language processing tasks. It's #awesome!\"\n",
        "\n",
        "# Tokenization\n",
        "print(\"\\n**Whitespace Tokenization:**\")\n",
        "tokens_ws = word_tokenize(text)\n",
        "print(tokens_ws)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFZxTVpz4_gH",
        "outputId": "199231eb-d0de-4743-c957-5cb2645a6f2b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Whitespace Tokenization:**\n",
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', 'tasks', '.', 'It', \"'s\", '#', 'awesome', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n**Punctuation-based Tokenization:**\")\n",
        "tokens_punct = wordpunct_tokenize(text)\n",
        "print(tokens_punct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqRLbncA5Bvs",
        "outputId": "a260d9b9-9900-4cb1-b476-ab5a2220e141"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Punctuation-based Tokenization:**\n",
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', 'tasks', '.', 'It', \"'\", 's', '#', 'awesome', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n**Treebank Tokenization:**\")\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "tokens_treebank = treebank_tokenizer.tokenize(text)\n",
        "print(tokens_treebank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jwp9g33B5LgI",
        "outputId": "fad80ccd-40b3-4686-def4-b476faa79816"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Treebank Tokenization:**\n",
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', 'tasks.', 'It', \"'s\", '#', 'awesome', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n**Tweet Tokenization:**\")\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tokens_tweet = tweet_tokenizer.tokenize(text)\n",
        "print(tokens_tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NzPCth55NuR",
        "outputId": "9c65a194-999c-49d5-d425-d4b1aad6241d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Tweet Tokenization:**\n",
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', 'tasks', '.', \"It's\", '#awesome', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "print(\"\\n**Porter Stemming:**\")\n",
        "porter_stemmer = PorterStemmer()\n",
        "stems_porter = [porter_stemmer.stem(token) for token in tokens_ws]\n",
        "print(stems_porter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up9BTr505Nwo",
        "outputId": "8d6b8f20-c753-4f58-c226-80c0e3832bad"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Porter Stemming:**\n",
            "['nltk', 'is', 'a', 'power', 'librari', 'for', 'natur', 'languag', 'process', 'task', '.', 'it', \"'s\", '#', 'awesom', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n**Snowball Stemming:**\")\n",
        "snowball_stemmer = SnowballStemmer(\"english\")  # Choose a language\n",
        "stems_snowball = [snowball_stemmer.stem(token) for token in tokens_ws]\n",
        "print(stems_snowball)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt15juST5Nzk",
        "outputId": "719b43a8-2526-4401-fbd1-188630682498"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Snowball Stemming:**\n",
            "['nltk', 'is', 'a', 'power', 'librari', 'for', 'natur', 'languag', 'process', 'task', '.', 'it', \"'s\", '#', 'awesom', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "print(\"\\n**Lemmatization:**\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(token) for token in tokens_ws]\n",
        "print(lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLVF3zJG5TkZ",
        "outputId": "495b4e7a-1799-49da-afae-6f5ca458d79a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Lemmatization:**\n",
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', 'task', '.', 'It', \"'s\", '#', 'awesome', '!']\n"
          ]
        }
      ]
    }
  ]
}