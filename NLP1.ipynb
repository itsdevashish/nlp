{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Name :- Devashish Mayur Potnis\n",
        "#Roll No :- 43557\n",
        "#Practical No :- 2"
      ],
      "metadata": {
        "id": "im0ksBmGEFDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')  # This is the missing resource\n",
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize, TreebankWordTokenizer, TweetTokenizer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09E0qj3k8Hr9",
        "outputId": "df8ace9d-7fd2-4ca3-c0a1-52c1abec16e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WADL9PYu8g_9",
        "outputId": "7a254a00-1d17-4fd7-a941-0cefea557f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"NLTK is a powerful library for natural language processing tasks. It's #awesome!\"\n",
        "\n",
        "# Tokenization\n",
        "print(\"\\n**Whitespace Tokenization:**\")\n",
        "tokens_ws = word_tokenize(text)\n",
        "print(tokens_ws)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e56TbwKC8Yl-",
        "outputId": "55c97739-a82c-420e-acfc-f936dc0181dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Whitespace Tokenization:**\n",
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', 'tasks', '.', 'It', \"'s\", '#', 'awesome', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n**Punctuation-based Tokenization:**\")\n",
        "tokens_punct = wordpunct_tokenize(text)\n",
        "print(tokens_punct)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqYMIxz18Yoq",
        "outputId": "c85d7eaf-e0d6-4d65-8f20-8a33e25c8b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Punctuation-based Tokenization:**\n",
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', 'tasks', '.', 'It', \"'\", 's', '#', 'awesome', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n**Treebank Tokenization:**\")\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "tokens_treebank = treebank_tokenizer.tokenize(text)\n",
        "print(tokens_treebank)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwYCDzuZ8Yrz",
        "outputId": "3a91c766-7a3c-4a80-f971-ce6fac2ba41e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Treebank Tokenization:**\n",
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', 'tasks.', 'It', \"'s\", '#', 'awesome', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n**Tweet Tokenization:**\")\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tokens_tweet = tweet_tokenizer.tokenize(text)\n",
        "print(tokens_tweet)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSLbdtCm8Yux",
        "outputId": "d12b5faf-69a3-42b7-b065-6b44eb17ac4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Tweet Tokenization:**\n",
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', 'tasks', '.', \"It's\", '#awesome', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "print(\"\\n**Porter Stemming:**\")\n",
        "porter_stemmer = PorterStemmer()\n",
        "stems_porter = [porter_stemmer.stem(token) for token in tokens_ws]\n",
        "print(stems_porter)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiLDATXC8YxP",
        "outputId": "ebb9a50c-6532-4356-ac71-f825edc4b003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Porter Stemming:**\n",
            "['nltk', 'is', 'a', 'power', 'librari', 'for', 'natur', 'languag', 'process', 'task', '.', 'it', \"'s\", '#', 'awesom', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n**Snowball Stemming:**\")\n",
        "snowball_stemmer = SnowballStemmer(\"english\")  # Choose a language\n",
        "stems_snowball = [snowball_stemmer.stem(token) for token in tokens_ws]\n",
        "print(stems_snowball)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SNM-knc8x_f",
        "outputId": "6045c146-1fd0-4da1-8f4c-064808e1ad06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Snowball Stemming:**\n",
            "['nltk', 'is', 'a', 'power', 'librari', 'for', 'natur', 'languag', 'process', 'task', '.', 'it', \"'s\", '#', 'awesom', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "print(\"\\n**Lemmatization:**\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(token) for token in tokens_ws]\n",
        "print(lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1-MnAb38zN2",
        "outputId": "007a5b56-748f-4f42-adf8-c4fceb12e937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Lemmatization:**\n",
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', 'task', '.', 'It', \"'s\", '#', 'awesome', '!']\n"
          ]
        }
      ]
    }
  ]
}