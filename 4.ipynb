{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G-MEvhEg9muq"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # Linear transformations\n",
        "        Q = self.query(query)\n",
        "        K = self.key(key)\n",
        "        V = self.value(value)\n",
        "\n",
        "        # Split into heads\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float('-1e20'))\n",
        "\n",
        "        attention = torch.nn.functional.softmax(energy, dim=-1)\n",
        "        x = torch.matmul(attention, V)\n",
        "\n",
        "        # Reshape and concatenate\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        x = x.view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # Final linear layer\n",
        "        x = self.fc_out(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Self-attention\n",
        "        attention = self.self_attention(x, x, x, mask)\n",
        "        x = x + self.dropout(attention)\n",
        "        x = self.layer_norm1(x)\n",
        "\n",
        "        # Feedforward\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = x + self.dropout(ffn_output)\n",
        "        x = self.layer_norm2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, ff_dim, n_layers, max_seq_length, dropout=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model, n_heads, ff_dim, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.position_embedding = nn.Embedding(max_seq_length, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        positions = torch.arange(0, x.size(1)).expand(x.size(0), x.size(1)).to(self.device)\n",
        "        x = x + self.position_embedding(positions)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "ff_dim = 2048\n",
        "n_layers = 6\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "\n",
        "# Create transformer encoder\n",
        "transformer_encoder = TransformerEncoder(d_model, n_heads, ff_dim, n_layers, max_seq_length, dropout)\n",
        "\n",
        "# Dummy input\n",
        "input_data = torch.rand((16, 100, d_model))\n",
        "\n",
        "# Mask for padding\n",
        "padding_mask = (input_data.sum(dim=-1) != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "# Forward pass\n",
        "output_data = transformer_encoder(input_data, padding_mask)\n",
        "print(output_data)\n",
        "print(\"Output shape:\", output_data.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xURrIyXF9tCo",
        "outputId": "5cd9e7b4-4848-4d9d-ebc8-fe1f8e568683"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.8408, -0.8154,  1.0487,  ...,  0.3487, -0.5811, -0.4857],\n",
            "         [-0.2406, -0.1920,  1.2893,  ..., -0.3320,  0.3077, -0.5451],\n",
            "         [-1.7497, -0.3533,  0.7075,  ..., -0.9475,  1.7996,  0.8380],\n",
            "         ...,\n",
            "         [-0.1005,  0.2460,  1.8928,  ...,  0.5881,  0.3590, -0.3711],\n",
            "         [-0.5257, -0.6494,  0.1706,  ...,  1.7171,  0.5511,  0.5390],\n",
            "         [-1.7992, -0.7236,  0.9918,  ..., -0.5738,  1.0991, -0.0445]],\n",
            "\n",
            "        [[ 0.0956, -0.5797,  0.5266,  ...,  0.2994, -0.9134,  0.3147],\n",
            "         [ 0.5751, -0.1986,  1.5392,  ..., -0.0737,  0.4916, -0.8520],\n",
            "         [-2.0388, -0.1143,  0.9389,  ..., -1.4413,  1.6410,  1.2478],\n",
            "         ...,\n",
            "         [-0.3299, -0.0159,  0.9269,  ..., -0.6512,  0.9223,  0.1962],\n",
            "         [-0.3567, -0.5883,  1.6152,  ...,  1.8126,  0.9458,  1.1106],\n",
            "         [-1.4371, -0.2394, -0.0975,  ..., -0.0598,  0.9240, -0.6363]],\n",
            "\n",
            "        [[-0.9640, -0.8538,  0.4637,  ...,  0.1433, -1.1513, -0.3659],\n",
            "         [ 1.0318, -0.6533,  1.5773,  ...,  0.3286,  0.4986, -0.3909],\n",
            "         [-1.6243, -0.9984,  1.0370,  ..., -1.2949,  1.2472,  1.6973],\n",
            "         ...,\n",
            "         [ 0.3175,  0.4934,  1.7175,  ...,  0.2346,  0.1652,  0.0345],\n",
            "         [-0.1027,  0.1614,  1.2445,  ...,  1.6465,  0.6721,  0.3388],\n",
            "         [-1.6528, -0.8491,  0.3587,  ...,  1.1846,  0.8041, -0.1685]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.2076, -0.7045,  0.9824,  ..., -0.2470, -1.2259, -0.2221],\n",
            "         [ 0.2664, -0.6562,  1.3604,  ..., -0.7709,  0.2270, -1.1820],\n",
            "         [-1.5301, -0.3325,  0.0075,  ..., -1.8791,  0.7260,  1.4410],\n",
            "         ...,\n",
            "         [ 0.9226,  1.4522,  1.4720,  ...,  0.1791,  0.8065, -0.7470],\n",
            "         [ 0.0153, -0.2580,  1.2599,  ...,  1.2269,  0.8508,  0.0179],\n",
            "         [-1.0760, -0.9526,  1.0102,  ..., -0.3109,  0.9849, -0.6965]],\n",
            "\n",
            "        [[ 0.5616, -0.5811,  0.3160,  ..., -1.1293, -0.8463,  0.0274],\n",
            "         [ 0.5296,  0.6053,  1.3844,  ..., -0.5581,  1.0027, -1.4350],\n",
            "         [-0.7292, -0.6857,  0.1968,  ..., -1.4119,  1.5609,  0.9190],\n",
            "         ...,\n",
            "         [ 0.3794,  1.0117,  1.6946,  ...,  0.5110, -0.0812, -0.1014],\n",
            "         [-0.1474, -0.3255,  1.5551,  ...,  1.1645,  0.5552,  0.5885],\n",
            "         [-1.5984, -1.2784,  0.9473,  ...,  0.1444,  1.0534,  0.0372]],\n",
            "\n",
            "        [[ 0.2127, -0.4311,  0.6624,  ...,  0.1149, -0.5572,  0.2417],\n",
            "         [-0.7553, -0.5787,  1.1483,  ..., -0.1023,  0.0685, -1.1045],\n",
            "         [-1.9035, -0.3082,  1.1138,  ..., -1.1892,  1.6146,  1.4417],\n",
            "         ...,\n",
            "         [ 0.0698,  0.2494,  1.4086,  ...,  0.5353, -0.3403, -0.6091],\n",
            "         [ 0.3040, -1.0323,  1.5974,  ...,  1.7898,  0.5551,  0.6962],\n",
            "         [-1.4752, -1.0204,  0.4725,  ...,  0.3693,  0.8265, -0.3765]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Output shape: torch.Size([16, 100, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a8wGtCol9zHa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}